{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcbefb5a-b092-41b7-bca0-8fc9594eb8bd",
   "metadata": {},
   "source": [
    "# Generating covariates for legislative districts\n",
    "\n",
    "This notebook is for generating certain covariates for part of a program evaluation I conducted in spring 2025. Specifically, I generate economic, demographic, and political covariates using spatial data to calculate overlaps between counties/precincts and legislative boundaries.\n",
    "\n",
    "The notebook does the following:\n",
    "- Grabs covariate data from BEA etc.\n",
    "- Pulls shape-files for CD, State legislature, counties, etc.\n",
    "- Generates CD-level covariates.\n",
    "- Generates SLDL-level covariates.\n",
    "- Generates SLDU-level covariates.\n",
    "\n",
    "*Note:* This is only a part of the script, since the latter part involves merging all the dataframes into another main dataframe based on the Vote-USA candidate list for the 2024 cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc10778-2c27-4551-8616-1b4bbf106ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements_covars.txt\n",
    "\n",
    "pandas\n",
    "scikit-learn\n",
    "matplotlib\n",
    "seaborn\n",
    "numpy\n",
    "morethemes\n",
    "requests\n",
    "beautifulsoup4\n",
    "# geopandas # had to install this via conda to deal with dependency issues\n",
    "openpyxl\n",
    "shapely\n",
    "thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539992cd-9b1e-407b-bf36-2fa0abdaea34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements_covars.txt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import morethemes as mt\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.validation import make_valid\n",
    "import random\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ca4fc-bf74-4b41-bbc8-9f48bc5e7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting graph theme\n",
    "mt.set_theme(\"monoblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe7ed5-3f24-4a56-b467-7cd4caa327e9",
   "metadata": {},
   "source": [
    "## Getting shape files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b2b04-3848-4e6a-9c26-764d324f4fbc",
   "metadata": {},
   "source": [
    "Now the heavy lift begins, creating the district-level covariates. We need to:\n",
    "- Download shape files from US Census for:\n",
    "  - Congressional districts\n",
    "  - State upper level districts\n",
    "  - State lower level districts\n",
    "  - US counties\n",
    "- Download 2020 precinct-level election data from Harvard Dataverse\n",
    "- Import county-level data from BEA, etc.\n",
    "- Generate district-level data using geopandas\n",
    "- Importing state-level data from BEA, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d5d6e-972d-4bf5-afd5-23d13381d081",
   "metadata": {},
   "source": [
    "### Congressional districts pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ebbe5-5389-4f11-8780-c6a16a625ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url for congressional district shape files\n",
    "url = \"https://www2.census.gov/geo/tiger/TIGER2024/CD/\"\n",
    "\n",
    "# sending a GET request to the URL and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# extracting all the file links, checking if 'href' exists\n",
    "file_links = [url + node.get(\"href\") for node in soup.find_all(\"a\") \n",
    "              if node.get(\"href\") and node.get(\"href\").endswith(\".zip\")]\n",
    "\n",
    "# creating dir\n",
    "os.makedirs(\"./_data/_cd/\", exist_ok=True)\n",
    "\n",
    "# downloading each file\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_cd/\", file_url.split(\"/\")[-1])\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    with requests.get(file_url, stream=True) as r:\n",
    "        r.raise_for_status()  # ensuring we get a valid response\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee38cf9-b7b3-4fb8-bb23-00e239bb9ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unzipping and deleting\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_cd/\", file_url.split(\"/\")[-1])\n",
    "    \n",
    "    # Check if it's a zip file and unzip it\n",
    "    if filename.endswith(\".zip\"):\n",
    "        # Create a directory for the extracted files based on the zip file's name (without the .zip extension)\n",
    "        extract_folder = os.path.join(\"./_data/_cd/\", file_url.split(\"/\")[-1].replace(\".zip\", \"\"))\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Unzipping {filename} into {extract_folder}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            # Extract all files into the newly created folder\n",
    "            zip_ref.extractall(extract_folder)\n",
    "        \n",
    "        # Delete the .zip file after extraction\n",
    "        print(f\"Deleting {filename}...\")\n",
    "        os.remove(filename)\n",
    "\n",
    "print(\"Unzip, and cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0c59e-a182-4106-b2f0-cb047db9efd8",
   "metadata": {},
   "source": [
    "### State legislature pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7888aec-5311-4376-95cd-45bbcda0feac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url for state legislature district lower shape files\n",
    "url = \"https://www2.census.gov/geo/tiger/TIGER2024/SLDL/\"\n",
    "\n",
    "# sending a GET request to the URL and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# extracting all the file links, checking if 'href' exists\n",
    "file_links = [url + node.get(\"href\") for node in soup.find_all(\"a\") \n",
    "              if node.get(\"href\") and node.get(\"href\").endswith(\".zip\")]\n",
    "\n",
    "# creating dir\n",
    "os.makedirs(\"./_data/_sldl/\", exist_ok=True)\n",
    "\n",
    "# downloading each file\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_sldl/\", file_url.split(\"/\")[-1])\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    with requests.get(file_url, stream=True) as r:\n",
    "        r.raise_for_status()  # ensuring we get a valid response\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# unzipping and deleting\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_sldl/\", file_url.split(\"/\")[-1])\n",
    "    \n",
    "    # Check if it's a zip file and unzip it\n",
    "    if filename.endswith(\".zip\"):\n",
    "        # Create a directory for the extracted files based on the zip file's name (without the .zip extension)\n",
    "        extract_folder = os.path.join(\"./_data/_sldl/\", file_url.split(\"/\")[-1].replace(\".zip\", \"\"))\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Unzipping {filename} into {extract_folder}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            # Extract all files into the newly created folder\n",
    "            zip_ref.extractall(extract_folder)\n",
    "        \n",
    "        # Delete the .zip file after extraction\n",
    "        print(f\"Deleting {filename}...\")\n",
    "        os.remove(filename)\n",
    "\n",
    "print(\"Unzip, and cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6111b6-4ee6-4aa9-95fd-189d334a2df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url for state legislature district upper shape files\n",
    "url = \"https://www2.census.gov/geo/tiger/TIGER2024/SLDU/\"\n",
    "\n",
    "# sending a GET request to the URL and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# extracting all the file links, checking if 'href' exists\n",
    "file_links = [url + node.get(\"href\") for node in soup.find_all(\"a\") \n",
    "              if node.get(\"href\") and node.get(\"href\").endswith(\".zip\")]\n",
    "\n",
    "# creating dir\n",
    "os.makedirs(\"./_data/_sldu/\", exist_ok=True)\n",
    "\n",
    "# downloading each file\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_sldu/\", file_url.split(\"/\")[-1])\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    with requests.get(file_url, stream=True) as r:\n",
    "        r.raise_for_status()  # ensuring we get a valid response\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# unzipping and deleting\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_sldu/\", file_url.split(\"/\")[-1])\n",
    "    \n",
    "    # Check if it's a zip file and unzip it\n",
    "    if filename.endswith(\".zip\"):\n",
    "        # Create a directory for the extracted files based on the zip file's name (without the .zip extension)\n",
    "        extract_folder = os.path.join(\"./_data/_sldu/\", file_url.split(\"/\")[-1].replace(\".zip\", \"\"))\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Unzipping {filename} into {extract_folder}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            # Extract all files into the newly created folder\n",
    "            zip_ref.extractall(extract_folder)\n",
    "        \n",
    "        # Delete the .zip file after extraction\n",
    "        print(f\"Deleting {filename}...\")\n",
    "        os.remove(filename)\n",
    "\n",
    "print(\"Unzip, and cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282b574-529d-4aaf-a7f7-843ddc1c882e",
   "metadata": {},
   "source": [
    "### US counties pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537935fd-681f-4830-94bc-8c1fb58830ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url for counties shape files\n",
    "url = \"https://www2.census.gov/geo/tiger/TIGER2024/COUNTY/\"\n",
    "\n",
    "# sending a GET request to the URL and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# extracting all the file links, checking if 'href' exists\n",
    "file_links = [url + node.get(\"href\") for node in soup.find_all(\"a\") \n",
    "              if node.get(\"href\") and node.get(\"href\").endswith(\".zip\")]\n",
    "\n",
    "# creating dir\n",
    "os.makedirs(\"./_data/_county/\", exist_ok=True)\n",
    "\n",
    "# downloading each file\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_county/\", file_url.split(\"/\")[-1])\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    with requests.get(file_url, stream=True) as r:\n",
    "        r.raise_for_status()  # ensuring we get a valid response\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# unzipping and deleting\n",
    "for file_url in file_links:\n",
    "    filename = os.path.join(\"./_data/_county/\", file_url.split(\"/\")[-1])\n",
    "    \n",
    "    # Check if it's a zip file and unzip it\n",
    "    if filename.endswith(\".zip\"):\n",
    "        # Create a directory for the extracted files based on the zip file's name (without the .zip extension)\n",
    "        extract_folder = os.path.join(\"./_data/_county/\", file_url.split(\"/\")[-1].replace(\".zip\", \"\"))\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Unzipping {filename} into {extract_folder}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            # Extract all files into the newly created folder\n",
    "            zip_ref.extractall(extract_folder)\n",
    "        \n",
    "        # Delete the .zip file after extraction\n",
    "        print(f\"Deleting {filename}...\")\n",
    "        os.remove(filename)\n",
    "\n",
    "print(\"Unzip, and cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40cbce-1376-43d2-a524-577d9d82a7ad",
   "metadata": {},
   "source": [
    "### Unzipping 2020 election results files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e869e8-eb1a-4d4a-9663-99e3f3f6c08f",
   "metadata": {},
   "source": [
    "I had to manually download the VEST 2020 precinct-level election results, so this is just a one-time operation to unzip and clean up the folder. You can find the data here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/K7760H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c9b0b-41be-4747-bbb3-fbd2f7362807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # dir\n",
    "# zip_dir = \"./_data/_election2020/\"\n",
    "# \n",
    "# # get all .zip files in the directory\n",
    "# zip_files = [f for f in os.listdir(zip_dir) if f.endswith(\".zip\")]\n",
    "# \n",
    "# # unzip each .zip file into a separate folder and delete the .zip afterward\n",
    "# for zip_file in zip_files:\n",
    "#     zip_file_path = os.path.join(zip_dir, zip_file)\n",
    "#     \n",
    "#     extract_folder = os.path.join(zip_dir, zip_file.replace(\".zip\", \"\"))\n",
    "#     os.makedirs(extract_folder, exist_ok=True)\n",
    "#     \n",
    "#     print(f\"Unzipping {zip_file_path} into {extract_folder}...\")\n",
    "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(extract_folder)\n",
    "#     \n",
    "#     print(f\"Deleting {zip_file_path}...\")\n",
    "#     os.remove(zip_file_path)\n",
    "# \n",
    "# print(\"Unzipping and cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42ec25-4eb2-4b22-b73f-cd3c1e2e8d2f",
   "metadata": {},
   "source": [
    "## Importing county- and state-level covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b5259-7227-4d1c-860c-24f454e4aef0",
   "metadata": {},
   "source": [
    "Here we need to import:\n",
    "- County GDP 2023\n",
    "- County GDPpc 2023\n",
    "- County urban 2020\n",
    "- States GDP 2023\n",
    "- State urban 2020\n",
    "- States GDP 2024 quarterly (ended up not using this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd844b-55ba-485a-a927-c6aeca91e5e1",
   "metadata": {},
   "source": [
    "### County GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709c761-ecb0-429d-ae4f-81b716d302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./_data/_covariates/county_gdp_2023.csv\", \n",
    "                 encoding=\"utf-8\", \n",
    "                 skiprows=3,\n",
    "                 skipfooter=12,\n",
    "                 engine='python',\n",
    "                 dtype={'GeoFips': str}\n",
    ")\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b8129-4016-4183-a931-7af18239f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up\n",
    "\n",
    "# only keeping real gdp\n",
    "df = df[df['LineCode'] == 1]\n",
    "\n",
    "# dropping unnecessary columns\n",
    "\n",
    "df = df.drop(columns=['LineCode', 'Description'])\n",
    "\n",
    "df = df.rename(columns={'2023': 'gdp_real'})\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "county_gdp = df.copy()\n",
    "print(county_gdp.info())\n",
    "print(county_gdp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71baac16-de77-4393-bb43-357abf390096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./_data/_covariates/county_gdppc_2023.csv\", \n",
    "                 encoding=\"utf-8\", \n",
    "                 skiprows=3,\n",
    "                skipfooter=20,\n",
    "                engine='python',\n",
    "                dtype={'GeoFips': str}\n",
    ")\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8394a-2e80-41bb-9bf9-7c1c6b756fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping description col\n",
    "df = df.drop(columns=['Description', 'GeoName'])\n",
    "\n",
    "# reshape wide the df\n",
    "df = df.pivot(index='GeoFips', columns='LineCode', values='2023')\n",
    "\n",
    "# renaming columns based on LineCode\n",
    "column_mapping = {\n",
    "    1: 'personal_income_2023',\n",
    "    2: 'population_2023',\n",
    "    3: 'personal_income_pc_2023'\n",
    "}\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# removing the index name\n",
    "df.columns.name = None\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62f8ef-3b43-47f9-a28c-a711143d4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to join the county gdp dfs\n",
    "county_gdp = county_gdp.merge(df, on=\"geofips\", how=\"left\")\n",
    "print(county_gdp.info())\n",
    "print(county_gdp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c351a8-cfcf-4f26-bbb7-21e26644e1ff",
   "metadata": {},
   "source": [
    "### County urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa729c2d-eb32-4ea8-83a6-467548f7371e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    './_data/_covariates/county_urban_2020.xlsx',\n",
    "    dtype={'STATE': str, 'COUNTY': str}\n",
    ")\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381a382-7604-45dd-a3ce-77bd4a1fefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column list:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d7feb-2676-4697-84b6-70966a9a6a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time to clean up\n",
    "\n",
    "# creating new geofips id\n",
    "df['geofips'] = df['STATE'].astype(str) + df['COUNTY'].astype(str)\n",
    "\n",
    "df = df[['geofips', 'POPDEN_COU', 'POPPCT_URB']]\n",
    "df.columns = df.columns.str.lower()\n",
    "df = df.rename(columns={'popden_cou': 'popden_cou_2020', 'poppct_urb': 'poppct_urb_2020'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e4071-3b45-426b-820e-c9c3fd6c1a35",
   "metadata": {},
   "source": [
    "### Joining county-level covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220a8ee-ed7f-4382-b6cf-87bb698ac50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging this into new county df\n",
    "\n",
    "county_covars = county_gdp.merge(df, on=\"geofips\", how=\"left\")\n",
    "county_covars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183adce-142e-4c45-8e22-1f6cc93544c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to recast to numeric\n",
    "exclude = ['geofips', 'geoname']\n",
    "\n",
    "for col in county_covars.columns:\n",
    "    if col not in exclude:\n",
    "        county_covars[col] = pd.to_numeric(county_covars[col], errors='coerce')\n",
    "\n",
    "# Optionally, check the data types to confirm conversion\n",
    "print(county_covars.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1cfaa-29f3-44a7-abff-44fc2f0a1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add in gross urban population number estimate for later calculations\n",
    "county_covars['urban'] = county_covars['population_2023'] * county_covars['poppct_urb_2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71600986-ef87-4baa-b346-c38104844d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "county_covars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033cc58-1a74-4568-9046-231797759283",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_covars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d08429-80cc-4c5e-bf20-3995ecf7cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of entries where geofips starts with '51'\n",
    "virginia_geofips_count = county_covars[county_covars['geofips'].str.startswith('51', na=False)].shape[0]\n",
    "\n",
    "# printing the result\n",
    "print(f\"Number of entries in county_covars where geofips starts with '51': {virginia_geofips_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb5ae3-cf76-4a35-a5e5-a6325b29de2d",
   "metadata": {},
   "source": [
    "### State GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fe384-edab-4415-84f9-7df0a59d0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./_data/_covariates/states_gdp_2023.csv\", \n",
    "                 encoding=\"utf-8\", \n",
    "                 skiprows=3,\n",
    "                 skipfooter=12,\n",
    "                 engine='python',\n",
    "                 dtype={'GeoFips': str}\n",
    ")\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab16e8-c9f0-48ce-9848-af381bd101cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping real gdp, personal income, and personal income pc\n",
    "df = df[df['LineCode'].isin({1.0, 5.0, 10.0})]\n",
    "\n",
    "# changing LineCode type\n",
    "df['LineCode'] = df['LineCode'].astype(int)\n",
    "\n",
    "# dropping description col\n",
    "df = df.drop(columns=['Description'])\n",
    "\n",
    "# reshape wide the df\n",
    "df = df.pivot(index=['GeoFips', 'GeoName'], columns='LineCode', values='2023')\n",
    "\n",
    "# renaming columns based on LineCode\n",
    "column_mapping = {\n",
    "    1: 'gdp_real',\n",
    "    5: 'personal_income_2023',\n",
    "    10: 'personal_income_pc_2023'\n",
    "}\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# removing the index name\n",
    "df.columns.name = None\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77f12d-dd60-44f0-b49c-233c096e19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_gdp_2023 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae54100-971a-4b89-a39a-eba3b926e770",
   "metadata": {},
   "source": [
    "### State urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0daff8-7542-4d75-abc5-14a792597009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    './_data/_covariates/state_urban.xlsx')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839e767-b6cb-4393-a0ef-b6a846b9915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b96295-1b9b-421f-b9e0-eab212a0f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[['STATE NAME', '2020 TOTAL POP', '2020 \\nURBAN POP', '2020 \\nRURAL POP', '2020 PCT URBAN POP']].copy()\n",
    "\n",
    "df_filtered = df_filtered.rename(columns={\n",
    "    'STATE NAME': 'geoname',\n",
    "    '2020 TOTAL POP': 'population_2023',\n",
    "    '2020 \\nURBAN POP': 'urban',\n",
    "    '2020 \\nRURAL POP': 'rural',\n",
    "    '2020 PCT URBAN POP': 'poppct_urb_2020'\n",
    "})\n",
    "\n",
    "df_filtered['poppct_urb_2020'] = df_filtered['poppct_urb_2020'] / 100\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf03a6-ecfc-4190-a1f7-5cf746f6f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_urban = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efec6ed-b924-4568-a9b5-fb8fac509b90",
   "metadata": {},
   "source": [
    "### State 2020 election results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5897a-6eec-4c84-8c13-a8f89512ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./_data/_covariates/us_president_state.csv\", \n",
    "                 encoding=\"utf-8\", \n",
    ")\n",
    "\n",
    "df = df[df['year'] == 2020]\n",
    "df = df[(df['candidate'] == \"BIDEN, JOSEPH R. JR\") | (df['candidate'] == \"TRUMP, DONALD J.\")]\n",
    "df = df[['state_po', 'candidate', 'candidatevotes']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1464c-28eb-4acd-b221-2991a164f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df.pivot_table(\n",
    "    index=['state_po'],\n",
    "    columns='candidate',\n",
    "    values='candidatevotes',\n",
    "    aggfunc='sum'  # in case there are duplicates\n",
    ").reset_index()\n",
    "\n",
    "# removing index name\n",
    "df_wide.columns.name = None\n",
    "\n",
    "# renaming candidate columns\n",
    "df_wide = df_wide.rename(columns={\n",
    "    'TRUMP, DONALD J.': 'G20PRERTRU',\n",
    "    'BIDEN, JOSEPH R. JR': 'G20PREDBID'\n",
    "})\n",
    "\n",
    "df_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa0470-6341-4ca6-af2a-32f555d19e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_election2020 = df_wide.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9ea72-9b4f-43c9-8940-ad8f7c685714",
   "metadata": {},
   "source": [
    "### Joining state covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8eeda-b6a1-46a1-a527-ec0af01ff9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_covars = state_gdp_2023.merge(state_urban, on=\"geoname\", how=\"left\")\n",
    "state_covars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69efc06-beb6-4c6e-85b7-9e81359bce30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to add in two-letter state code\n",
    "states = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n",
    "    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
    "    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
    "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
    "    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
    "    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n",
    "}\n",
    "\n",
    "state_covars['state_code'] = state_covars['geoname'].map(states)\n",
    "state_covars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b9d68-1de0-4c03-a532-a38872050b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging in 2020 election results\n",
    "state_covars = state_covars.merge(state_election2020, left_on=\"state_code\", right_on=\"state_po\", how=\"left\")\n",
    "print(state_covars.info())\n",
    "print(state_covars.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8e984-2b65-4ec1-bb4c-1046702dfcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states_final = state_covars.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fff29-7c6c-4669-8383-980486367ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now exporting to CSV\n",
    "df_states_final.to_csv(\"./_data/_clean/states_covariates_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed82366-6617-4a2f-881b-fcfda6c0e89e",
   "metadata": {},
   "source": [
    "### State GDP 2024 quarterly\n",
    "\n",
    "I ended up not using this for the final analysis, but I'm leaving the code in here for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d9c81-5379-4ff4-9f53-03cd3ab66d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./_data/_covariates/states_gdp_2024quarterly.csv\", \n",
    "                 encoding=\"utf-8\", \n",
    "                 skiprows=3,\n",
    "                skipfooter=3,\n",
    "                engine='python',\n",
    "                dtype={'GeoFips': str}\n",
    ")\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8906e-7bec-450d-9e44-5f3cbd1abf35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[df['LineCode'] == 1]\n",
    "\n",
    "# dropping unnecessary columns\n",
    "\n",
    "df = df.drop(columns=['GeoName', 'LineCode', 'Description'])\n",
    "\n",
    "# reshaping\n",
    "df = df.melt(id_vars=['GeoFips'],\n",
    "             value_vars=['2024:Q1', '2024:Q2', '2024:Q3', '2024:Q4'],\n",
    "             var_name='quarter_2024',\n",
    "             value_name='gdp_real')\n",
    "\n",
    "# removing uppercase in column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# cleaning up the quarter variable so it's just an integer\n",
    "df['quarter_2024'] = df['quarter_2024'].str[-1].astype(int)\n",
    "\n",
    "df = df.sort_values(by=['geofips', 'quarter_2024'], ascending=[True, True])\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499eb8f-e49e-47c9-8c44-7925c6f84b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_gdp_2024q = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a362013-1b5c-4acf-8983-4ca724d02df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now exporting to CSV\n",
    "state_gdp_2024q.to_csv(\"./_data/_clean/states_gdp_2024q.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de685f-4b6d-49e2-b9a7-306059a44bb5",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfdc5b-538b-498c-802b-b2130fbb6654",
   "metadata": {},
   "source": [
    "Now we need to set up the data pipeline to estimate the GDP per district based on county-level data. Reminder that we need to do this for:\n",
    "- Congressional districts\n",
    "- State lower level districts\n",
    "- State upper level districts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0bcc45-6360-4e01-aba9-18e86f9bd9c5",
   "metadata": {},
   "source": [
    "### Loading county shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fea7f4-192c-449d-a1ae-4650eec1cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_shapefile_path = './_data/_county/tl_2024_us_county/tl_2024_us_county.shp'\n",
    "counties_gdf = gpd.read_file(county_shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12de0a-0a21-42b2-b336-5f4b9e563f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counties_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc09990-779c-40f9-8b40-26c10a7f07ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counties_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586eb40-bd5e-4ddc-ba8a-59830434e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0de1eb-8bac-4707-b877-71064335333b",
   "metadata": {},
   "source": [
    "### Loading precinct shape files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af4212-14f3-45c7-8165-b7a9001fe63b",
   "metadata": {},
   "source": [
    "Note here that the Voting and Election Science Team had to do a lot of work to fit election results to the precinct-level for many states due to the pandemic. In the instances of Kentucky and New Jersey, they provided two different estimates. I decided to keep the second estimates (VTD), since those made further calculations to apportion votes to precincts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81dbce3-ebe0-4528-af5b-e040664bd697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the VEST shape files do not have a column for STATEFIPS ID\n",
    "state_fips_dict = {\n",
    "    'al': '01', 'ak': '02', 'az': '04', 'ar': '05', 'ca': '06', \n",
    "    'co': '08', 'ct': '09', 'de': '10', 'fl': '12', 'ga': '13',\n",
    "    'hi': '15', 'id': '16', 'il': '17', 'in': '18', 'ia': '19', \n",
    "    'ks': '20', 'ky': '21', 'la': '22', 'me': '23', 'md': '24', \n",
    "    'ma': '25', 'mi': '26', 'mn': '27', 'ms': '28', 'mo': '29', \n",
    "    'mt': '30', 'ne': '31', 'nv': '32', 'nh': '33', 'nj': '34', \n",
    "    'nm': '35', 'ny': '36', 'nc': '37', 'nd': '38', 'oh': '39', \n",
    "    'ok': '40', 'or': '41', 'pa': '42', 'ri': '44', 'sc': '45', \n",
    "    'sd': '46', 'tn': '47', 'tx': '48', 'ut': '49', 'vt': '50', \n",
    "    'va': '51', 'wa': '53', 'wv': '54', 'wi': '55', 'wy': '56'\n",
    "}\n",
    "\n",
    "# initalizing an empty GeoDataFrame for congressional districts\n",
    "# need to initialize geometry column and set CRS because there was mismatch between the state shape files\n",
    "gdf = gpd.GeoDataFrame(columns=['geometry', 'STATEFP'], crs=\"EPSG:5070\")\n",
    "\n",
    "# path\n",
    "folder_path = './_data/_election2020/'\n",
    "\n",
    "# loading congressional district shapefiles\n",
    "for group_folder in os.listdir(folder_path):\n",
    "    group_path = os.path.join(folder_path, group_folder)\n",
    "    if os.path.isdir(group_path):\n",
    "        # extracting the two-letter state abbreviation from the folder name\n",
    "        state_abbr = group_folder[:2]\n",
    "        # checking to see if the state abbreviation exists in the dictionary\n",
    "        if state_abbr in state_fips_dict:\n",
    "            state_fips = state_fips_dict[state_abbr]\n",
    "        else:\n",
    "            print(f\"State abbreviation {state_abbr} not found in dictionary!\")\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(group_path):\n",
    "            if file.endswith('.shp'):\n",
    "                shapefile_path = os.path.join(group_path, file)\n",
    "                group_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "                # adding STATEFIPS column\n",
    "                group_gdf['STATEFP'] = state_fips\n",
    "\n",
    "                # reproject if CRS is different\n",
    "                if group_gdf.crs != gdf.crs:\n",
    "                    group_gdf = group_gdf.to_crs(gdf.crs)\n",
    "\n",
    "                # append to master gdf\n",
    "                gdf = pd.concat([gdf, group_gdf], ignore_index=True)\n",
    "\n",
    "# assigning the df\n",
    "precincts_gdf = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7f1f5-7d97-4c1d-a96f-1193088fe488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the geometries in precincts_gdf are valid\n",
    "invalid_geometries = precincts_gdf[~precincts_gdf.is_valid]\n",
    "\n",
    "# Print the invalid geometries, if any\n",
    "print(invalid_geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66f315-b938-471c-b821-a894e2ee79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at Iowa\n",
    "iowa_precincts = precincts_gdf[precincts_gdf['STATEFP'] == '19']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "iowa_precincts.plot(ax=ax, color='lightblue', edgecolor='black')\n",
    "ax.set_title('Iowa Precincts', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1032dbd-f2ba-4edf-8616-fe1cc83b20fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invalid_geometries = iowa_precincts[~iowa_precincts.is_valid]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "invalid_geometries.plot(ax=ax, color='lightblue', edgecolor='black')\n",
    "ax.set_title('Iowa Precincts', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d077b-80ad-45d6-aa03-f28ce62871e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iowa_precincts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e5bb6-c833-4ba2-8c1c-f44f0ab46fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the geometries in precincts_gdf are valid\n",
    "invalid_geometries = precincts_gdf[~precincts_gdf.is_valid]\n",
    "\n",
    "# Print the invalid geometries, if any\n",
    "print(invalid_geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f8d5a-2919-42a7-a416-29a2f5829eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precincts_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4e8f1-9cc1-4895-9ee4-ca6f30c8288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter precincts_gdf to keep only the first 7 columns by index\n",
    "precincts_gdf = precincts_gdf.iloc[:, :7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d5cfe-b897-4862-89ab-6c9495ee2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precincts_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc185f4e-4f75-4494-9c3a-2ab0a031f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precincts_gdf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c3753-a3a7-4c8c-be6a-550806feca29",
   "metadata": {},
   "source": [
    "### Virginia issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aace47a-b230-4be0-9167-a5758a6f13ec",
   "metadata": {},
   "source": [
    "There is a huge discrepancy in how Virginia data are handled between BEA and US Census, so we have to make some imputations for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bc4cd-a5bf-41ec-92f1-9b9cc3561126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ID values from both dfs\n",
    "geofips_covars = set(county_covars['geofips'].dropna())\n",
    "geofips_counties = set(counties_gdf['GEOID'].dropna())\n",
    "\n",
    "# filtering for IDs that start with '51'\n",
    "geofips_covars_va = {geofips for geofips in geofips_covars if geofips.startswith('51')}\n",
    "geofips_counties_va = {geofips for geofips in geofips_counties if geofips.startswith('51')}\n",
    "\n",
    "# counties in county_covars but not in counties_gdf\n",
    "missing_in_counties_gdf_va = sorted(geofips_covars_va - geofips_counties_va)\n",
    "\n",
    "# creating df for missing entries with geonames\n",
    "missing_counties_df = county_covars[county_covars['geofips'].isin(missing_in_counties_gdf_va)][['geofips', 'geoname']]\n",
    "\n",
    "print(\"Virginia counties in county_covars but not in counties_gdf with geoname:\")\n",
    "print(missing_counties_df)\n",
    "print(f\"Count: {len(missing_counties_df)}\")\n",
    "\n",
    "# counties in counties_gdf but not in county_covars\n",
    "missing_in_county_covars_va = sorted(geofips_counties_va - geofips_covars_va)\n",
    "print(\"\\nVirginia counties in counties_gdf but not in county_covars:\")\n",
    "print(missing_in_county_covars_va)\n",
    "print(f\"Count: {len(missing_in_county_covars_va)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d2b08-a591-4d6f-b1cf-60e74710c632",
   "metadata": {},
   "source": [
    "The problem here is that the BEA data combines data from lots of county+city pairs, including Charlottesville (540) and Albemarle county (003), which are referred to as 901. So I'm going to see what I can do about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307a7ce-500d-41ff-a097-41cc0f0a26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_entries = county_covars[(county_covars['geofips'].astype(int) > 51900) & (county_covars['geofips'].astype(int) < 52000)]\n",
    "print(\"Aggregated entries in county_covars:\")\n",
    "print(aggregated_entries[['geofips', 'geoname', 'gdp_real']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800b734-bfe8-4422-b1d9-0f25a309c89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counties_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7ec89-efc7-42c9-84e5-3152dc2ed9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge county GDP data with county shapefiles\n",
    "counties_gdf = counties_gdf.merge(county_covars, left_on='GEOID', right_on='geofips', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794a2b7-0354-4c3a-befb-5ba296e40088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counties_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb86b9-6eaa-4904-9a96-bcf4e0d2af80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter counties_gdf to include only Virginia entries\n",
    "virginia_counties_gdf = counties_gdf[(counties_gdf['STATEFP'] == '51') & (counties_gdf['gdp_real'].isnull())]\n",
    "print(virginia_counties_gdf)\n",
    "\n",
    "# normalize names and stripping whitespace for matching purposes\n",
    "virginia_counties_gdf['NAME'] = virginia_counties_gdf['NAME'].str.lower().str.strip()\n",
    "aggregated_entries['geoname'] = aggregated_entries['geoname'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961f7d5-215b-417c-b304-366499232f4a",
   "metadata": {},
   "source": [
    "Now I'm going to try and assign the various covariates from the groups to the subcomponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0586074-43d3-423d-acc3-aaa0ea700d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars to distribute\n",
    "vars = ['gdp_real', 'personal_income_2023', 'population_2023', 'urban']\n",
    "\n",
    "# calculating the area of each subcomponent in virginia_counties_gdf\n",
    "virginia_counties_gdf.loc[:, 'area'] = virginia_counties_gdf.geometry.area\n",
    "\n",
    "# processing each aggregated entry\n",
    "for index, row in aggregated_entries.iterrows():\n",
    "    # Find subcomponents by checking if the county name is in the geoname string\n",
    "    subcomponents = virginia_counties_gdf[virginia_counties_gdf['NAME'].apply(lambda name: name in row['geoname'])]\n",
    "    \n",
    "    print(f\"Subcomponents for {row['geoname']}:\")\n",
    "    print(subcomponents[['GEOID', 'NAME']])\n",
    "    \n",
    "    # Calculate total area\n",
    "    total_area = subcomponents['area'].sum()\n",
    "    \n",
    "    # Distribute each variable\n",
    "    for var in vars:\n",
    "        if var in row:\n",
    "            for sub_index, sub_row in subcomponents.iterrows():\n",
    "                proportion = sub_row['area'] / total_area\n",
    "                distributed_value = row[var] * proportion\n",
    "                # ensure the column exists in counties_gdf\n",
    "                if var not in counties_gdf.columns:\n",
    "                    counties_gdf[var] = 0\n",
    "                counties_gdf.loc[sub_index, var] = distributed_value\n",
    "\n",
    "print(\"Looking at the number of non-null values in the relevant covariates:\", counties_gdf.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5f31f-0ae3-41a7-ac3a-dca0e0dde7da",
   "metadata": {},
   "source": [
    "I'm not sure what some of these warnings are about, but this is sort of working. A couple of incorrect (superfluous) matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ef06d-653d-4f94-a356-d1faa51d2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at Charlottesville and Albemarle\n",
    "# Print rows from counties_gdf where GEOID is 51540 or 51003\n",
    "print(\"Rows from counties_gdf with GEOID 51540 and 51003:\")\n",
    "print(counties_gdf[counties_gdf['GEOID'].isin(['51540', '51003'])])\n",
    "\n",
    "# Print the row from county_covars where geofips is 51901\n",
    "print(\"\\nRow from county_covars with geofips 51901:\")\n",
    "print(county_covars[county_covars['geofips'] == '51901'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed223bd-7eb4-4103-ae97-1b51b740d64a",
   "metadata": {},
   "source": [
    "This is problematic, since it assumes an even distribution of GDP across the space, not taking into account density etc. But not sure how to account for that. An alternative would be to aportion GDP by population instead, so if I find fully disaggregated county data on populations I can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ed56f-80a0-4608-8c16-8252d84c0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(county_covars['gdp_real'].describe())\n",
    "print(counties_gdf['gdp_real'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8b680-d6e1-4b40-9bd8-bbda39e5f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fcb4e-f9ec-440a-be87-6f53a8fbd388",
   "metadata": {},
   "source": [
    "### Congressional districts shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406c6fa-ddbc-4707-a5d3-a3ed07c86672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalizing an empty GeoDataFrame for congressional districts\n",
    "gdf = gpd.GeoDataFrame()\n",
    "\n",
    "# loading congressional district shapefiles\n",
    "folder_path = './_data/_cd/'\n",
    "for group_folder in os.listdir(folder_path):\n",
    "    group_path = os.path.join(folder_path, group_folder)\n",
    "    if os.path.isdir(group_path):\n",
    "        for file in os.listdir(group_path):\n",
    "            if file.endswith('.shp'):\n",
    "                shapefile_path = os.path.join(group_path, file)\n",
    "                group_gdf = gpd.read_file(shapefile_path)\n",
    "                gdf = pd.concat([gdf, group_gdf], ignore_index=True)\n",
    "\n",
    "# assigning the df\n",
    "cd_gdf = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e77dd7-9ec5-479a-a658-6df9ca3e9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cd_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55299ac9-b8f3-43fa-bb05-f83c38d2719f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d4969-dff7-4fc6-9136-479841fc220a",
   "metadata": {},
   "source": [
    "### State lower districts shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ba91f-71a6-4081-85d0-d6ac939232ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalizing an empty GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame()\n",
    "\n",
    "# path\n",
    "folder_path = './_data/_sldl/'\n",
    "\n",
    "# loading sldl shapefiles\n",
    "for group_folder in os.listdir(folder_path):\n",
    "    group_path = os.path.join(folder_path, group_folder)\n",
    "    if os.path.isdir(group_path):\n",
    "        for file in os.listdir(group_path):\n",
    "            if file.endswith('.shp'):\n",
    "                shapefile_path = os.path.join(group_path, file)\n",
    "                group_gdf = gpd.read_file(shapefile_path)\n",
    "                gdf = pd.concat([gdf, group_gdf], ignore_index=True)\n",
    "\n",
    "# assigning the df\n",
    "sldl_gdf = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9636c40-53ac-4e4d-be90-c64f186a2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sldl_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f597a4-a774-48ce-a43d-4d62045515f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sldl_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f449ab7-0e3c-40ce-ac92-8cd02e565e95",
   "metadata": {},
   "source": [
    "### State upper districts shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fad56-97a4-439b-ab99-b13b0f0cadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalizing an empty GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame()\n",
    "\n",
    "# path\n",
    "folder_path = './_data/_sldu/'\n",
    "\n",
    "# loading sldl shapefiles\n",
    "for group_folder in os.listdir(folder_path):\n",
    "    group_path = os.path.join(folder_path, group_folder)\n",
    "    if os.path.isdir(group_path):\n",
    "        for file in os.listdir(group_path):\n",
    "            if file.endswith('.shp'):\n",
    "                shapefile_path = os.path.join(group_path, file)\n",
    "                group_gdf = gpd.read_file(shapefile_path)\n",
    "                gdf = pd.concat([gdf, group_gdf], ignore_index=True)\n",
    "\n",
    "# assigning the df\n",
    "sldu_gdf = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdefd1-15fb-4aa4-9f87-4437d76975cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sldu_gdf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62038d32-2dd4-4c61-b83a-dfb373be9477",
   "metadata": {},
   "source": [
    "### Function for converting to district level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f77318-8627-47e2-9398-c1c45304cdbd",
   "metadata": {},
   "source": [
    "Now creating the function to process each unit (e.g., CD) in a group (e.g., state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd3323-a64e-4e79-bf84-625c44158618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(input=None, target=None, group=None, vars=None):\n",
    "    if input is None or target is None or group is None or vars is None:\n",
    "        raise ValueError(\"All parameters (input, target, group, vars) must be provided.\")\n",
    "    \n",
    "    print(f\"Processing group: {group}\")\n",
    "    print(f\"Initial CRS of input: {input.crs}\")\n",
    "    print(f\"Initial CRS of target: {target.crs}\")\n",
    "    \n",
    "    # setting the appropriate CRS based on the group\n",
    "    if group == '02':  # Alaska\n",
    "        crs = \"EPSG:3338\"\n",
    "    elif group == '15':  # Hawaii\n",
    "        crs = \"EPSG:3563\"\n",
    "    else:  # Contiguous US\n",
    "        crs = \"EPSG:5070\"\n",
    "    \n",
    "    # re-projecting to state-specific CRS\n",
    "    if input.crs != crs:\n",
    "        input = input.to_crs(crs)\n",
    "    if target.crs != crs:\n",
    "        target = target.to_crs(crs)\n",
    "    \n",
    "    print(f\"Re-projected CRS of input: {input.crs}\")\n",
    "    print(f\"Re-projected CRS of target: {target.crs}\")\n",
    "    \n",
    "    # performing spatial join\n",
    "    overlap_gdf = gpd.sjoin(input, target, how='inner', predicate='intersects')\n",
    "    print(f\"Number of overlapping geometries: {len(overlap_gdf)}\")\n",
    "    \n",
    "    # calculating area of overlap\n",
    "    overlap_gdf['intersection'] = overlap_gdf.apply(\n",
    "        lambda row: input.loc[row.name, 'geometry'].intersection(\n",
    "            target.loc[row['index_right'], 'geometry']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    overlap_gdf['intersection_area'] = overlap_gdf['intersection'].area\n",
    "    overlap_gdf['county_area'] = overlap_gdf['geometry'].area\n",
    "    overlap_gdf['proportion'] = overlap_gdf['intersection_area'] / overlap_gdf['county_area']\n",
    "    \n",
    "    print(\"Sample of overlap_gdf after intersection calculation:\")\n",
    "    print(overlap_gdf[['intersection_area', 'county_area', 'proportion']].head())\n",
    "    \n",
    "    # initializing df to store results\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # calculating contributions for each variable\n",
    "    for var in vars:\n",
    "        if var in overlap_gdf.columns:\n",
    "            print(f\"Calculating contributions for variable: {var}\")\n",
    "            overlap_gdf[f'{var}_contribution'] = overlap_gdf[var] * overlap_gdf['proportion']\n",
    "            # aggregate by target group\n",
    "            aggregated = overlap_gdf.groupby('GEOID_right')[f'{var}_contribution'].sum().reset_index()\n",
    "            aggregated.columns = ['GEOID', var]\n",
    "            # merge results\n",
    "            if results.empty:\n",
    "                results = aggregated\n",
    "            else:\n",
    "                results = results.merge(aggregated, on='GEOID', how='outer')\n",
    "    \n",
    "    print(\"Final results sample:\")\n",
    "    print(results.head())\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb14685-dd35-412f-a0f9-1a370094a313",
   "metadata": {},
   "source": [
    "### Generating CD covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320f489-7942-462a-9c2f-12740d571229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['gdp_real', 'personal_income_2023', 'population_2023', 'urban']\n",
    "df_covars = pd.DataFrame()\n",
    "\n",
    "for state_fips in counties_gdf['STATEFP'].unique():\n",
    "    input_gdf = counties_gdf[counties_gdf['STATEFP'] == state_fips]\n",
    "    unit_gdf = cd_gdf[cd_gdf['STATEFP'] == state_fips]\n",
    "    \n",
    "    unit_covars = process_group(input=input_gdf, target=unit_gdf, group=state_fips, vars=vars)\n",
    "    df_covars = pd.concat([df_covars, unit_covars], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc61aa2-4ff1-436f-9c52-229520923933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8aa41a-59b0-4d45-8d3c-7d3e3da11908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277e615-3f06-4861-8127-c22b0db1cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45042230-6f9b-40ed-8244-b956027ea0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd_covars = df_covars.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41314e5c-fa68-4cc4-be75-9b483db68b84",
   "metadata": {},
   "source": [
    "### Validating pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb94ec-69fe-4c46-ac37-60349b4a8dee",
   "metadata": {},
   "source": [
    "Now I need to make sure this actually worked the way it was supposed to. I'll start by checking the state-level sum of real GDP from the county covars dataframe and the output dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7b14c-961f-451c-92c3-5918eb67020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract state FIPS from 'geofips' in county_covars\n",
    "county_covars['state_fips'] = county_covars['geofips'].str[:2]\n",
    "\n",
    "# extract state FIPS from 'GEOID' in df_covars\n",
    "df_covars['state_fips'] = df_covars['GEOID'].str[:2]\n",
    "\n",
    "# calculate state-level GDP sums in county_covars\n",
    "state_gdp_county = county_covars.groupby('state_fips')['gdp_real'].sum().reset_index()\n",
    "state_gdp_county.columns = ['state_fips', 'total_gdp_county']\n",
    "\n",
    "# calculate state-level GDP sums in df_covars\n",
    "state_gdp_cd = df_covars.groupby('state_fips')['gdp_real'].sum().reset_index()\n",
    "state_gdp_cd.columns = ['state_fips', 'total_gdp_cd']\n",
    "\n",
    "# merge and calculate difference\n",
    "state_gdp_comparison = pd.merge(state_gdp_county, state_gdp_cd, on='state_fips', how='outer')\n",
    "state_gdp_comparison['gdp_difference'] = (\n",
    "    (state_gdp_comparison['total_gdp_county'] - state_gdp_comparison['total_gdp_cd']) \n",
    "    / state_gdp_comparison['total_gdp_county']\n",
    ").round(2)\n",
    "\n",
    "print(state_gdp_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48235ba8-cec8-44eb-a092-6cba99b91e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"County CRS:\", counties_gdf.crs)\n",
    "print(\"District CRS:\", cd_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46f168-31ad-450d-8d79-9e4779df6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the virginia map\n",
    "counties_gdf[counties_gdf['STATEFP'] == '51'].plot()\n",
    "cd_gdf[cd_gdf['STATEFP'] == '51'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23b624-710c-44f2-b5ed-4aab97f0180f",
   "metadata": {},
   "source": [
    "Let's take a look at some congressional districts and compare to their associated counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee92ac-cf62-4638-8377-2142765f29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rand_map(counties_gdf, cd_gdf, df_covars):\n",
    "    # random district\n",
    "    random_district_id = random.choice(df_covars['GEOID'].unique())\n",
    "    \n",
    "    # GDP of the selected district\n",
    "    district_gdp = df_covars[df_covars['GEOID'] == random_district_id]['gdp_real'].values[0]\n",
    "    \n",
    "    # geometry of the selected district\n",
    "    district_geometry = cd_gdf[cd_gdf['GEOID'] == random_district_id].geometry.values[0]\n",
    "    \n",
    "    # counties that overlap with the selected district\n",
    "    overlapping_counties = counties_gdf[counties_gdf.intersects(district_geometry)].copy()\n",
    "    \n",
    "    # calculating the intersection area and the percentage overlap\n",
    "    overlapping_counties['intersection'] = overlapping_counties['geometry'].intersection(district_geometry)\n",
    "    overlapping_counties['intersection_area'] = overlapping_counties['intersection'].area\n",
    "    overlapping_counties['county_area'] = overlapping_counties['geometry'].area\n",
    "    overlapping_counties['overlap_percentage'] = (overlapping_counties['intersection_area'] / overlapping_counties['county_area']) * 100\n",
    "    \n",
    "    # calculating and printing the GDP of the overlapping counties\n",
    "    overlapping_counties_gdp = overlapping_counties['gdp_real'].sum()\n",
    "    \n",
    "    print(f\"Randomly selected district ID: {random_district_id}\")\n",
    "    print(f\"District GDP: {district_gdp}\")\n",
    "    print(f\"Overlapping counties GDP: {overlapping_counties_gdp}\")\n",
    "    print(\"Overlapping counties:\")\n",
    "    print(overlapping_counties[['NAME', 'gdp_real', 'overlap_percentage']])\n",
    "    \n",
    "    # plotting the district and overlapping counties\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cd_gdf[cd_gdf['GEOID'] == random_district_id].plot(ax=ax, color='lightblue', edgecolor='black', label='District')\n",
    "    overlapping_counties.plot(ax=ax, color='none', edgecolor='red', label='Overlapping Counties')\n",
    "    \n",
    "    plt.title(f\"District {random_district_id} with Overlapping Counties\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# now run the corrected version\n",
    "sample_rand_map(counties_gdf, cd_gdf, df_covars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238eb073-aeff-4180-8729-cabbf67f5298",
   "metadata": {},
   "source": [
    "Checked a couple of different outputs, including the Virginia 2nd CD, and they looked right. Note that the geometry tends to include counties with minimal overlap, including sometimes from neighboring states. This shouldn't be an issue, as their calculated overlap is basically zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b745e-0ba7-4847-bc6d-ed4b5d2cca6e",
   "metadata": {},
   "source": [
    "### Generating SLDL covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a305083-3263-4ba6-a81d-837ab61387be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sldl_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c214d3-698a-4979-9374-53e75d07b5b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['gdp_real', 'personal_income_2023', 'population_2023', 'urban']\n",
    "df_covars = pd.DataFrame()\n",
    "\n",
    "for state_fips in counties_gdf['STATEFP'].unique():\n",
    "    input_gdf = counties_gdf[counties_gdf['STATEFP'] == state_fips]\n",
    "    unit_gdf = sldl_gdf[sldl_gdf['STATEFP'] == state_fips]\n",
    "    \n",
    "    # checking if the unit_gdf is empty, which indicates no lower house data for the state\n",
    "    if unit_gdf.empty:\n",
    "        print(f\"Skipping state {state_fips} as it has no lower house data.\")\n",
    "        continue\n",
    "    \n",
    "    unit_covars = process_group(input=input_gdf, target=unit_gdf, group=state_fips, vars=vars)\n",
    "    df_covars = pd.concat([df_covars, unit_covars], ignore_index=True)\n",
    "\n",
    "print(\"Final combined results:\")\n",
    "print(df_covars.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b496e5-47fa-4132-999b-085d5e141425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de055a7d-b8a3-464f-8e2d-0a6aab65f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a39778-ac54-4a30-a1b7-f4fde0f24bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sldl_covars = df_covars.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631b6ed-aef7-4fe2-8f14-3d2dbfa1e649",
   "metadata": {},
   "source": [
    "### Generating SLDU covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42185d7c-3173-49c3-9e82-e50d3990fe5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['gdp_real', 'personal_income_2023', 'population_2023', 'urban']\n",
    "df_covars = pd.DataFrame()\n",
    "\n",
    "for state_fips in counties_gdf['STATEFP'].unique():\n",
    "    input_gdf = counties_gdf[counties_gdf['STATEFP'] == state_fips]\n",
    "    unit_gdf = sldu_gdf[sldu_gdf['STATEFP'] == state_fips]\n",
    "    \n",
    "    # checking if the unit_gdf is empty, which indicates no upper house data for the state\n",
    "    if unit_gdf.empty:\n",
    "        print(f\"Skipping state {state_fips} as it has no upper house data.\")\n",
    "        continue\n",
    "    \n",
    "    unit_covars = process_group(input=input_gdf, target=unit_gdf, group=state_fips, vars=vars)\n",
    "    df_covars = pd.concat([df_covars, unit_covars], ignore_index=True)\n",
    "\n",
    "print(\"Final combined results:\")\n",
    "print(df_covars.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2947633-0f98-4d74-ae42-a20529abe7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d7c91-9389-47bc-8340-a4a2cc395d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a1159-28c2-45a2-9cc4-05abe43849f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sldu_covars = df_covars.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fc4c2-b67f-46bc-907a-4ea21c4416cf",
   "metadata": {},
   "source": [
    "### Generating CD 2020 election results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ccd88-7067-40ff-a1e4-8b00c2c5a2a1",
   "metadata": {},
   "source": [
    "I had to redo the processing function because there are some precincts in the VEST data with bad geometry and the quick fixes simply do not work. After a lot of troubleshooting and trial and error, I ended up having to delete the overlaps with bad geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6db99f-d8b3-4921-9dd7-64a7544ad09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group_precincts(input=None, target=None, group=None, vars=None):\n",
    "    if input is None or target is None or group is None or vars is None:\n",
    "        raise ValueError(\"All parameters (input, target, group, vars) must be provided.\")\n",
    "    \n",
    "    print(f\"Processing group: {group}\")\n",
    "    print(f\"Initial CRS of input: {input.crs}\")\n",
    "    print(f\"Initial CRS of target: {target.crs}\")\n",
    "    \n",
    "    # setting the appropriate CRS based on the group\n",
    "    if group == '02':  # Alaska\n",
    "        crs = \"EPSG:3338\"\n",
    "    elif group == '15':  # Hawaii\n",
    "        crs = \"EPSG:3563\"\n",
    "    else:  # Contiguous US\n",
    "        crs = \"EPSG:5070\"\n",
    "    \n",
    "    # re-projecting to state-specific CRS\n",
    "    if input.crs != crs:\n",
    "        input = input.to_crs(crs)\n",
    "    if target.crs != crs:\n",
    "        target = target.to_crs(crs)\n",
    "    \n",
    "    print(f\"Re-projected CRS of input: {input.crs}\")\n",
    "    print(f\"Re-projected CRS of target: {target.crs}\")\n",
    "\n",
    "    # ensure geometries are valid just before performing the intersection\n",
    "    precincts_gdf['geometry'] = precincts_gdf['geometry'].apply(lambda geom: make_valid(geom) if not geom.is_valid else geom)\n",
    "    cd_gdf['geometry'] = cd_gdf['geometry'].apply(lambda geom: make_valid(geom) if not geom.is_valid else geom)\n",
    "\n",
    "    # performing spatial join\n",
    "    overlap_gdf = gpd.sjoin(input, target, how='inner', predicate='intersects')\n",
    "    print(f\"Number of overlapping geometries: {len(overlap_gdf)}\")\n",
    "\n",
    "    # drorpping invalid geometries from overlap_gdf\n",
    "    overlap_gdf = overlap_gdf[overlap_gdf.is_valid]\n",
    "    print(f\"Number of valid overlapping geometries after dropping invalid ones: {len(overlap_gdf)}\")\n",
    "\n",
    "    # checking to make sure GEOID is in there\n",
    "    print(overlap_gdf.columns)\n",
    "    \n",
    "    # calculating area of overlap\n",
    "    overlap_gdf['intersection'] = overlap_gdf.apply(\n",
    "        lambda row: input.loc[row.name, 'geometry'].intersection(\n",
    "            target.loc[row['index_right'], 'geometry']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    overlap_gdf['intersection_area'] = overlap_gdf['intersection'].area\n",
    "    overlap_gdf['county_area'] = overlap_gdf['geometry'].area\n",
    "    overlap_gdf['proportion'] = overlap_gdf['intersection_area'] / overlap_gdf['county_area']\n",
    "    \n",
    "    print(\"Sample of overlap_gdf after intersection calculation:\")\n",
    "    print(overlap_gdf[['intersection_area', 'county_area', 'proportion']].head())\n",
    "    \n",
    "    # initializing df to store results\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # calculating contributions for each variable\n",
    "    for var in vars:\n",
    "        if var in overlap_gdf.columns:\n",
    "            print(f\"Calculating contributions for variable: {var}\")\n",
    "            overlap_gdf[f'{var}_contribution'] = overlap_gdf[var] * overlap_gdf['proportion']\n",
    "            # aggregate by target group\n",
    "            aggregated = overlap_gdf.groupby('GEOID')[f'{var}_contribution'].sum().reset_index()\n",
    "            aggregated.columns = ['GEOID', var]\n",
    "            # merge results\n",
    "            if results.empty:\n",
    "                results = aggregated\n",
    "            else:\n",
    "                results = results.merge(aggregated, on='GEOID', how='outer')\n",
    "    \n",
    "    print(\"Final results sample:\")\n",
    "    print(results.head())\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6fb66-82e0-41c1-863e-5ee372752e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['G20PRERTRU',\n",
    "       'G20PREDBID']\n",
    "df_election2020 = pd.DataFrame()\n",
    "\n",
    "for state_fips in precincts_gdf['STATEFP'].unique():\n",
    "    input_gdf = precincts_gdf[precincts_gdf['STATEFP'] == state_fips]\n",
    "    unit_gdf = cd_gdf[cd_gdf['STATEFP'] == state_fips]\n",
    "    \n",
    "    unit_covars = process_group_precincts(input=input_gdf, target=unit_gdf, group=state_fips, vars=vars)\n",
    "    df_election2020 = pd.concat([df_election2020, unit_covars], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3bb10-cebf-4cb9-bff9-4335284fa722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_election2020.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b861f-fc99-408e-97a6-4469f8a2d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_election2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea267558-b650-47f8-9796-dae485eaa31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract state FIPS from 'GEOID' in df_election2020\n",
    "df_election2020['state_fips'] = df_election2020['GEOID'].str[:2]\n",
    "\n",
    "# calculate state-level G20PREDBID sums in precincts_gdf (using STATEFP directly)\n",
    "state_gdp_precincts = precincts_gdf.groupby('STATEFP')['G20PREDBID'].sum().reset_index()\n",
    "state_gdp_precincts.columns = ['state_fips', 'total_G20PREDBID_precincts']\n",
    "\n",
    "# calculate state-level G20PREDBID sums in df_election2020\n",
    "state_gdp_cd = df_election2020.groupby('state_fips')['G20PREDBID'].sum().reset_index()\n",
    "state_gdp_cd.columns = ['state_fips', 'total_G20PREDBID_cd']\n",
    "\n",
    "# merge and calculate difference\n",
    "state_gdp_comparison = pd.merge(state_gdp_precincts, state_gdp_cd, on='state_fips', how='outer')\n",
    "state_gdp_comparison['G20PREDBID_difference'] = (\n",
    "    (state_gdp_comparison['total_G20PREDBID_precincts'] - state_gdp_comparison['total_G20PREDBID_cd']) \n",
    "    / state_gdp_comparison['total_G20PREDBID_precincts']\n",
    ").round(2)\n",
    "\n",
    "print(state_gdp_comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b342cf-87c8-4818-ae81-189018455e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_election2020['G20PREDBID'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e326a-2f46-42b2-866c-8eb97a7f5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "precincts_gdf['G20PREDBID'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bcf958-fccf-4344-ba8d-511a8b5681a7",
   "metadata": {},
   "source": [
    "It seems we lost about 7k votes for Biden with the deletion of precincts with invalid geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b706408-142d-45fa-9cb9-7f1701fd0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd_election2020 = df_election2020.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182b3ce-549c-45eb-a172-e02cc5b8c11c",
   "metadata": {},
   "source": [
    "### Exporting CD level covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b19b1b-bf4f-4d1e-86fa-3697753aaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd_covars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a67b63-baf9-44be-a2e6-182a868d2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_election2020.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4afef-9f68-4217-94fb-df45ee914975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd_final = df_cd_covars.merge(df_cd_election2020[['GEOID', 'G20PRERTRU', 'G20PREDBID']], how='left', on='GEOID')\n",
    "print(df_cd_final.info())\n",
    "print(df_cd_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135aa3c2-b3d9-46e0-a52d-3b7479cdabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now exporting to csvcsv\n",
    "df_cd_final.to_csv(\"./_data/_clean/cd_covariates_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194f00f-551c-473b-865f-a241bed13602",
   "metadata": {},
   "source": [
    "### Generating SLDL 2020 election results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa876c5-2e98-435d-833b-121bfd618744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['G20PRERTRU',\n",
    "       'G20PREDBID']\n",
    "df_election2020 = pd.DataFrame()\n",
    "\n",
    "for state_fips in precincts_gdf['STATEFP'].unique():\n",
    "    input_gdf = precincts_gdf[precincts_gdf['STATEFP'] == state_fips]\n",
    "    unit_gdf = sldl_gdf[sldl_gdf['STATEFP'] == state_fips]\n",
    "\n",
    "    # checking if the unit_gdf is empty, which indicates no lower house data for the state\n",
    "    if unit_gdf.empty:\n",
    "            print(f\"Skipping state {state_fips} as it has no lower house data.\")\n",
    "            continue\n",
    "    \n",
    "    unit_covars = process_group_precincts(input=input_gdf, target=unit_gdf, group=state_fips, vars=vars)\n",
    "    df_election2020 = pd.concat([df_election2020, unit_covars], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa245ad-bc81-403f-ac37-8a2003727bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_election2020.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87f700-ce7c-4306-bba9-0c11f2a6aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sldl_election2020 = df_election2020.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f215efb-0801-405f-ad95-1d46e92a1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sldl_final = df_sldl_covars.merge(df_sldl_election2020[['GEOID', 'G20PRERTRU', 'G20PREDBID']], how='left', on='GEOID')\n",
    "print(df_sldl_final.info())\n",
    "print(df_sldl_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97967f8f-e010-406b-9a1a-5b90a4fdab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to merge in the name ID variable from the gdf here to better match with Vote USA\n",
    "sldl_gdf_filtered = sldl_gdf[['GEOID', 'NAMELSAD']]\n",
    "\n",
    "# merge the 'NAMELSAD' column into df_sldl_final based on the 'GEOID' column\n",
    "df_sldl_final = df_sldl_final.merge(\n",
    "    sldl_gdf_filtered,\n",
    "    on='GEOID', \n",
    "    how='left'  # Use a left join to retain all rows from df_sldl_final\n",
    ")\n",
    "\n",
    "# checking if the merge was successful\n",
    "df_sldl_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac251ad3-191d-4464-a75a-2d1e62a541d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now exporting to csvcsv\n",
    "df_sldl_final.to_csv(\"./_data/_clean/sldl_covariates_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05895430-43a1-4b69-940a-2bdd04090944",
   "metadata": {},
   "source": [
    "### Generating SLDU 2020 election results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e087c6-dcba-4459-b561-c466fcfabc92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['G20PRERTRU',\n",
    "       'G20PREDBID']\n",
    "df_election2020 = pd.DataFrame()\n",
    "\n",
    "for state_fips in precincts_gdf['STATEFP'].unique():\n",
    "    input_gdf = precincts_gdf[precincts_gdf['STATEFP'] == state_fips]\n",
    "    unit_gdf = sldu_gdf[sldu_gdf['STATEFP'] == state_fips]\n",
    "\n",
    "    # checking if the unit_gdf is empty\n",
    "    if unit_gdf.empty:\n",
    "            print(f\"Skipping state {state_fips} as it has no upper house data.\")\n",
    "            continue\n",
    "    \n",
    "    unit_covars = process_group_precincts(input=input_gdf, target=unit_gdf, group=state_fips, vars=vars)\n",
    "    df_election2020 = pd.concat([df_election2020, unit_covars], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68d9ea-c051-406d-acfb-db651b519a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_election2020.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63efbc7-328a-44f9-9ff0-6ab81f87735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sldu_election2020 = df_election2020.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c3ff7-acd6-4fc0-bac3-3eaab5132cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sldu_final = df_sldu_covars.merge(df_sldu_election2020[['GEOID', 'G20PRERTRU', 'G20PREDBID']], how='left', on='GEOID')\n",
    "print(df_sldu_final.info())\n",
    "print(df_sldu_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915bed5-9a45-4f6b-98f8-d9909ced288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to merge in the name ID variable from the gdf here to better match with Vote USA\n",
    "sldu_gdf_filtered = sldu_gdf[['GEOID', 'NAMELSAD']]\n",
    "\n",
    "# merge the 'NAMELSAD' column into df_sldl_final based on the 'GEOID' column\n",
    "df_sldu_final = df_sldu_final.merge(\n",
    "    sldu_gdf_filtered,\n",
    "    on='GEOID', \n",
    "    how='left'  # Use a left join to retain all rows from df_sldl_final\n",
    ")\n",
    "\n",
    "# checking if the merge was successful\n",
    "df_sldu_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e281167-a18a-4d66-865a-f858c0037f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now exporting to csvcsv\n",
    "df_sldu_final.to_csv(\"./_data/_clean/sldu_covariates_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
